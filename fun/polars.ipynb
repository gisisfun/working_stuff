{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "optical-steal",
   "metadata": {},
   "source": [
    "\n",
    "https://pythonspeed.com/articles/polars-memory-pandas/\n",
    "\n",
    "\n",
    "# Why Polars uses less memory than Pandas\n",
    "\n",
    "*by Itamar Turner-Trauring*\n",
    "*Last updated 12 Jan 2023, originally created 11 Jan 2023*\n",
    "\n",
    "Processing large amounts of data with Pandas can be difficult; it’s quite easy to run out of memory and either slow down or crash. The Polars dataframe library is a potential solution.\n",
    "\n",
    "While Polars is mostly known for running faster than Pandas, if you use it right it can sometimes also significantly reduce memory usage compared to Pandas. In particular, certain techniques that you need to do manually in Pandas can be done automatically in Polars, allowing you to process large datasets without using as much memory—and with less work on your side!\n",
    "\n",
    "This does require you to use the correct Polars APIs. And it won’t solve all your problems, even if it does make your life easier.\n",
    "\n",
    "In this article we’ll:\n",
    "\n",
    "1. See how we can optimize Pandas memory usage with a little work.\n",
    "2. See how Polars can, in some cases, use these techniques automatically.\n",
    "3. Note at least some of the ways you will need to intervene manually to reduce memory usage.\n",
    " \n",
    "## An example: going from naive Pandas to memory-optimized Pandas\n",
    "\n",
    "To help understand the way Polars can help reduce memory usage compared to Pandas, we’ll start with a concrete example and implement it in Pandas. We are going to look at recorded bus route times for the Boston area’s transit authority, the MBTA, and try to find bus routes that are extra slow.\n",
    "\n",
    "We’ll use data from 2022, [available here](https://mbta-massdot.opendata.arcgis.com/datasets/MassDOT::mbta-bus-arrival-departure-times-2022/about) in the form of CSVs. Each month’s data is a single CSV of about 300MB; we’ll be looking at the data from May.\n",
    "\n",
    "First we’ll implement a particular query with a naive Pandas implementation, and then with a more optimized but still Pandas implementation. In the next major section we’ll switch to Polars.\n",
    "\n",
    "\n",
    "\n",
    "### Step 1: A more efficient memory representation and file format\n",
    "Here’s a sample of the data, with some of the columns omitted:\n",
    "\n",
    "|service_ date|\troute_ id|\tdirection_ id|\tstandard_ type|\tscheduled|\tscheduled_ headway|\theadway|\n",
    "--------------|----------|---------------|----------------|----------|--------------------|--------|\n",
    "|2022-05-01\t|“01”|\t“Inbound”|\t“Schedule”|\t“1900-01-01 06:05:00.000”|\tNA\t|NA|\n",
    "|2022-05-01\t|“01”|\t“Inbound”|\t“Schedule”|\t“1900-01-01 06:25:00.000”|\tNA\t|NA|\n",
    "|2022-05-01|\t“01”|\t“Inbound”|\t“Headway”|\t“1900-01-01 06:25:00.000”|\t“1200”|\t“841”|\n",
    "|2022-05-01|\t“01”|\t“Inbound”|\t“Schedule”|\t“1900-01-01 06:29:00.000”|\tNA|\tNA|\n",
    "|2022-05-01|\t“01”|\t“Inbound”|\t“Schedule”|\t“1900-01-01 06:30:00.000”|\tNA|\tNA|\n",
    "\n",
    "```\n",
    "Inbound vs Outbound means going towards or from Boston, a city which is also known as “The Hub”.\n",
    "\n",
    "```\n",
    "\n",
    "The first thing to notice is that many of these columns can be represented using a more memory-efficient data type, without losing any information. See this article on reducing Pandas memory usage with compression for more details.\n",
    "\n",
    " - Instead of strings, service_date, actual and scheduled_headway can be timestamps. In the real world I’d do more work to ensure they’re actually dates or time-of-days, but for the limited example we’ll be working on a timestamp will suffice.\n",
    " \n",
    " - Instead of strings, route_id, direction_id, and standard_type can be categoricals.\n",
    "We may wish to process the file multiple times, for example to try different queries. In that case we won’t want data type conversion to happen after loading, we’d ideally have the data stored on disk in a way that remembers the data types we’d like to use. CSV does not qualify, it’s basically just a pile of strings.\n",
    "\n",
    "In addition, loading a CSV can be slow, with lots of parsing involved. [A better alternative to CSVs is the Parquet data format](https://pythonspeed.com/articles/pandas-load-less-data/): it has an actual concept of data types that is similar to Pandas, and it is faster to load.\n",
    "\n",
    "As our first step, then, we will load the CSV, choose better column types, and write the result to a Parquet file:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "harmful-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "files = os.listdir('Data/MBTA_Bus_Arrival_Departure_Times_2022')\n",
    "#fred = 'Data/MBTA_Bus_Arrival_Departure_Times_2022/MBTA-Bus-Arrival-Departure-Times_2022-01.csv'\n",
    "def read_files_to_parquet(file):\n",
    "    csv_path = os.path.join('Data','MBTA_Bus_Arrival_Departure_Times_2022',file)\n",
    "    parquet_path = csv_path.replace(\".csv\", \".parquet\")\n",
    "    \n",
    "    df = pd.read_csv(\n",
    "        csv_path,\n",
    "        dtype={\"route_id\": \"string\"},\n",
    "        parse_dates=[\"service_date\", \"scheduled\", \"actual\"])\n",
    "           \n",
    "    for categorical_col in [\"route_id\", \"direction_id\", \"point_type\", \"standard_type\"]:\n",
    "        df[categorical_col] = df[categorical_col].astype(\"category\")\n",
    "\n",
    "    df.to_parquet(parquet_path)\n",
    "    \n",
    "for file in files:\n",
    "    if \".csv\" in file:\n",
    "        read_files_to_parquet(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-power",
   "metadata": {},
   "source": [
    "As an added bonus, Parquet uses compression: the new file is 20MB, compared to 300MB for the CSV. Keep in mind this is just the on-disk size. The data will have to be uncompressed before loading into memory, so on-disk compression doesn’t help with memory usage.\n",
    "\n",
    "### Step 2: Finding slow bus routes with a naive Pandas implementation\n",
    "In order to find slow bus routes, we’re going to focus on “headways”: how often a particular bus route arrives. If we look at the sample data above, we can see that inbound bus 1 is supposed to arrive every 1200 seconds, but on May 1st it actually arrived more quickly, with a difference of 841 seconds. Not all rows have headway information; we only want rows where standard_type is Headway.\n",
    "\n",
    "Here’s our algorithm:\n",
    "\n",
    "1. Get rid of all rows that aren’t headway information.\n",
    "2. alculate the ratio of actual headway to expected headway; if it’s bigger than 1, that means the bus arrived late.\n",
    "3. For every pair of route number and direction (inbound/outbound), pick the median headway ratio for the month.\n",
    "4. Find the 5 route pairs with the worst median ratio.\n",
    "This is probably a bad way to find slow buses, but we’re just using this as an example, so that’s fine.\n",
    "\n",
    "Here’s a first pass, a naive implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "round-rapid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       headway_ratio\n",
      "route_id direction_id               \n",
      "108      Outbound           2.900000\n",
      "88       Outbound           1.680000\n",
      "83       Outbound           1.565000\n",
      "134      Outbound           1.431111\n",
      "         Inbound            1.346667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def find_worst_headways():\n",
    "    # Load the data:\n",
    "    data = pd.read_parquet(os.path.join('Data',\n",
    "                                        'MBTA_Bus_Arrival_Departure_Times_2022',\n",
    "                                        'MBTA-Bus-Arrival-Departure-Times_2022-05.parquet'))\n",
    "    # Filter down to headway points only:MBTA-Bus-Arrival-Departure-Times_2022-05    \n",
    "    data = data[data[\"standard_type\"] == \"Headway\"]\n",
    "    # Calculate ratio of actual headway to expected headway:\n",
    "    data[\"headway_ratio\"] = (\n",
    "        data[\"headway\"] / data[\"scheduled_headway\"]\n",
    "    )\n",
    "    # Group by route and direction (Inbound/Outbound):\n",
    "    by_route = data.groupby([\"route_id\", \"direction_id\"])\n",
    "    # Find median headway ratio for each route:\n",
    "    median_headway = by_route[[\"headway_ratio\"]].median()\n",
    "    # Return the worst 5 routes:\n",
    "    return median_headway.nlargest(\n",
    "        5, columns=[\"headway_ratio\"]\n",
    "    )\n",
    "\n",
    "print(find_worst_headways())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-problem",
   "metadata": {},
   "source": [
    "```\n",
    "For some reason one route ID is blank. I haven’t investigated why because this is just an example, but perhaps the input data is malformed. As long as our later implementations give the same results, this doesn’t really matter for this article, we’re comparing apples to apples. In the Polars implementation it comes out as 134, with all other results being the same.\n",
    "```\n",
    "\n",
    "By running the program using `/usr/bin/time -v`, we can see  [RSS (resident) memory usage](https://pythonspeed.com/articles/measuring-memory-python/max), and [wallclock time and CPU time](https://pythonspeed.com/articles/blocking-cpu-or-io/):\n",
    "```\n",
    "User time (seconds): 0.84\n",
    "System time (seconds): 1.33\n",
    "Percent of CPU this job got: 491%\n",
    "Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.44\n",
    "Maximum resident set size (kbytes): 909500\n",
    "```\n",
    "\n",
    "Pandas isn’t parallelized, but the Parquet loading library (in this case Arrow) can take advantage of multiple CPUs.\n",
    "\n",
    "### Step 3: Investigating memory usage\n",
    "We’ve learned that our naive Pandas implementation used 909MB of memory. That’s a lot! So next we’ll measure the sources of memory usage using the [Sciagraph performance and memory profiler](https://sciagraph.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-syria",
   "metadata": {},
   "source": [
    "```\n",
    "This report claims 1.2GB of memory allocated; previously we saw that max resident memory was 900MB. The difference is due to measuring different things.\n",
    "```\n",
    "\n",
    "Here’s where most of the memory was allocated:\n",
    "\n",
    " - 100MB from filtering down to headway rows only.\n",
    " - 75MB from calculating the median.\n",
    " - 1000MB in the Arrow library, which is used to load the data. This is not Python code, and Sciagraph doesn’t yet show native callstacks for memory allocations, so it’s not clear exactly which part of loading the data is responsible.\n",
    "\n",
    "Clearly we want to focus on the last item, but we also have less details there. We could switch to the Memray memory profiler, which does give native (C) callstacks. However, a little thought will suggest at least part of the problem, and the obvious next step. Our current processing involves loading lots of data and then throwing much of it away.\n",
    "\n",
    "In particular, we:\n",
    "\n",
    "Load all the data; this is where Arrow gets involved and allocates a huge amount of memory.\n",
    "Drop many rows, specifically those that don’t have headway data.\n",
    "Ignore many columns of data which we’re not using in this query.\n",
    "Chunking or batching is one of the basic techniques for reducing memory usage. If we loaded the data in chunks, rather than all at once, we could filter the data on a chunk by chunk basis. Then we could merge the much-smaller chunks and run our logic on a much smaller amount of data.\n",
    "\n",
    "### Step 4: A more optimized Pandas implementation\n",
    "Here’s an implementation based on our new insight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fluid-consumption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       headway_ratio\n",
      "route_id direction_id               \n",
      "108      Outbound           2.900000\n",
      "88       Outbound           1.680000\n",
      "83       Outbound           1.565000\n",
      "134      Outbound           1.431111\n",
      "         Inbound            1.346667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def find_worst_headways():\n",
    "    # Load the data in chunks:\n",
    "    chunks = []\n",
    "    parquet_file = pq.ParquetFile(\n",
    "        os.path.join('Data',\n",
    "                     'MBTA_Bus_Arrival_Departure_Times_2022',\n",
    "                     'MBTA-Bus-Arrival-Departure-Times_2022-05.parquet')\n",
    "    )\n",
    "    for batch in parquet_file.iter_batches():\n",
    "        chunk = batch.to_pandas()\n",
    "        del batch\n",
    "        # Calculate headway ratio:\n",
    "        chunk[\"headway_ratio\"] = (\n",
    "            chunk[\"headway\"] / chunk[\"scheduled_headway\"]\n",
    "        )\n",
    "        # Store the columns we care about for this chunk:\n",
    "        chunks.append(chunk[\n",
    "            [\"route_id\", \"direction_id\", \"headway_ratio\"]\n",
    "        ])\n",
    "    del parquet_file\n",
    "\n",
    "    # Concatenate into one big DataFrame.\n",
    "    # Not ideal, involves two copies in memory at once...\n",
    "    data = pd.concat(chunks)\n",
    "    del chunks\n",
    "\n",
    "    # Group by route and direction (Inbound/Outbound):\n",
    "    by_route = data.groupby([\"route_id\", \"direction_id\"])\n",
    "    # Find median day's headway ratio for each route:\n",
    "    median_headway = by_route[[\"headway_ratio\"]].median()\n",
    "    # Return the worst 5 routes:\n",
    "    return median_headway.nlargest(\n",
    "        5, columns=[\"headway_ratio\"]\n",
    "    )\n",
    "\n",
    "print(find_worst_headways())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-restriction",
   "metadata": {},
   "source": [
    "The output is the same, but it uses much less memory:\n",
    "```\n",
    "User time (seconds): 1.10\n",
    "System time (seconds): 1.26\n",
    "Percent of CPU this job got: 348%\n",
    "Elapsed (wall clock) time (h:mm:ss or m:ss): 0:00.68\n",
    "Maximum resident set size (kbytes): 364172\n",
    "```\n",
    "We’ve gone from 900MB max resident memory to 360MB, a very nice improvement.\n",
    "\n",
    "### Step 5: Try swapping out PyArrow for fastparquet\n",
    "In our original memory profiling we saw that PyArrow was responsible for the bulk of allocated memory, as part of loading the Parquet file. Pandas can also load Parquet files with a different library called fastparquet, so we can switch both our naive and optimized versions to use that and see how it impacts memory usage.\n",
    "\n",
    "Here’s what we changed in the naive version:\n",
    "```\n",
    "# ...\n",
    "data = pd.read_parquet(\"MBTA-2022-05.parquet\",\n",
    "                       engine=\"fastparquet\")\n",
    "# ...\n",
    "And the optimized version:\n",
    "\n",
    "import pandas as pd\n",
    "import fastparquet as pq\n",
    "\n",
    "def find_worst_headways():\n",
    "    # Load the data in chunks:\n",
    "    chunks = []\n",
    "    parquet_file = pq.ParquetFile(\"MBTA-2022-05.parquet\")\n",
    "    for chunk in parquet_file.iter_row_groups():\n",
    "        # Calculate headway ratio:\n",
    "        chunk[\"headway_ratio\"] = (\n",
    "            chunk[\"headway\"] / chunk[\"scheduled_headway\"]\n",
    "        )\n",
    "        # ...\n",
    "        \n",
    "```\n",
    "When measuring memory usage of the resulting code, it turns out that Fastparquet uses far less memory than PyArrow for the naive version. But the optimized version is actually worse! I could spend the time trying to figure out why but that’s probably too much of a digression. At a guess, it’s loading the whole file, and we need to adjust the row groups to be smaller when creating the Parquet file if we want to get any benefit from chunking.\n",
    "\n",
    "Here’s a summary of our various implementation so far:\n",
    "\n",
    "|Implementation\t|Max Resident RAM|\tElapsed seconds|\tCPU seconds|\n",
    "|---------------|-----------------|----------------|---------------|\n",
    "|Pandas naive (PyArrow)|\t909MB|\t0.44 secs|\t2.17 secs|\n",
    "|Pandas optimized (PyArrow)|\t364MB|\t0.68 secs|\t2.36 secs|\n",
    "|Pandas naive (Fastparquet)|\t400MB|\t0.71 secs|\t2.07 secs|\n",
    "|Pandas “optimized” (Fastparquet)|\t460MB|\t0.71 secs|\t2.04 secs|\n",
    "\n",
    "## Lazy processing, lazy programmer: using less memory with Polars\n",
    "So far we’ve learned that a manually-implemented batching implementation can, at least with PyArrow, reduce memory usage in Pandas. Annoyingly, this requires us to manually restructure how the data is represented and loaded. Ideally our library would do that for us, but unfortunately this is not possible when using Pandas.\n",
    "\n",
    "Pandas is a eager API: you tell it to do something, and it immediately does it. So if you tell it to load a file, it will load all of it into memory; it has no way of knowing you intend to drop half the data on the next line of code.\n",
    "\n",
    "The alternative is a lazy API that allows you to string together a series of operations—loading, filtering, aggregating, transforming—without actually doing any work. After creating this series of operations, you can then separately tell the library to execute the whole thing.\n",
    "\n",
    "A smart lazy library can then look at all the operations, and come up with an optimized execution plan that takes into account everything you plan to do—and everything you plan not to do. For example:\n",
    "\n",
    "If you’re not touching a column at all, there is no need to load it into memory.\n",
    "If batching is possible, the library could do batching for you automatically.\n",
    "Polars is an alternative to Pandas with many benefits, like multi-core processing—and it supports both eager and lazy APIs. Using the lazy API can mean lower memory usage without having to do extra work to manually batch data processing.\n",
    "\n",
    "### Our Polars implementation\n",
    "Polars’ eager loading APIs tend to start with read_*, and the lazy loading APIs start with scan_*. Here’s what our code looks like when reimplemented with the Polars lazy API, specifically scan_parquet():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "extraordinary-kentucky",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ParamSpec' from 'typing_extensions' (/usr/lib/python3/dist-packages/typing_extensions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-e12c36f40f7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpolars\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mheadways_sorted_worst_first\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Load the data lazily:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     data = pl.scan_parquet(os.path.join('Data',\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/polars/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"polars binary missing!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpolars\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpolars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_info\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpolars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/polars/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwarnings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpolars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m __all__ = [\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/polars/internals/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0m_scan_parquet_fsspec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpolars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatched\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchedCsvReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpolars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataframe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrap_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m from polars.internals.expr import (\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/polars/internals/batched.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpolars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatatypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mN_INFER_DEFAULT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPolarsDataType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpy_type_to_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpolars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_aliases\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCsvEncoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m from polars.utils import (\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0m_prepare_row_count_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0m_process_null_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/polars/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParamSpec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeGuard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtyping_extensions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParamSpec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeGuard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;31m# note: reversed views don't match as instances of MappingView\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ParamSpec' from 'typing_extensions' (/usr/lib/python3/dist-packages/typing_extensions.py)"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "def headways_sorted_worst_first():\n",
    "    # Load the data lazily:\n",
    "    data = pl.scan_parquet(os.path.join('Data',\n",
    "                     'MBTA_Bus_Arrival_Departure_Times_2022',\n",
    "                     'MBTA-Bus-Arrival-Departure-Times_2022-05.parquet'))\n",
    "    # Filter down to headway points only and then select\n",
    "    # the data we need:\n",
    "    data = data.filter(\n",
    "        pl.col(\"standard_type\") == \"Headway\"\n",
    "    ).select(\n",
    "        [\n",
    "            pl.col(\"route_id\"),\n",
    "            pl.col(\"direction_id\"),\n",
    "            pl.col(\"headway\") / pl.col(\"scheduled_headway\"),\n",
    "        ]\n",
    "    )\n",
    "    # Group by route and direction (Inbound/Outbound):\n",
    "    by_route = data.groupby([\"route_id\", \"direction_id\"])\n",
    "    # Find median headway ratio for each route:\n",
    "    median_headway = by_route.agg(\n",
    "        pl.col(\"headway\").median()\n",
    "    )\n",
    "    # There's no nlargest() method, so instead just sort\n",
    "    # in descending order:\n",
    "    return median_headway.sort(\"headway\", reverse=True)\n",
    "\n",
    "# Create the query:\n",
    "query = headways_sorted_worst_first()\n",
    "# Actually run the query:\n",
    "result = query.collect()\n",
    "# Print the 5 worst headways:\n",
    "print(result[:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-church",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-cleaners",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-defeat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-oasis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-deadline",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
